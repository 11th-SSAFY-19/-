# 분산 키-값 저장소 설계
**❓ 키-값 저장소란?**
- 키-값 데이터베이스라고도 불리며 비 관계형 데이터베이스이다.
- 이 저장소에 저장되는 값은 고유 식별자를 키로 가져야 한다.
  - 해당 키에 매달린 값은 오로지 키를 통해서만 접근이 가능!
- 성능상의 이유로 키는 짧을 수록 좋다(해시 값을 사용하는 경우가 있음!)
- 예시로는 Amazon Dynamo, Memcached, Redis와 같은 것들이 있다.

## 키-값 저장소 설계
키-값 저장소를 설계하기 위해서는 
1. 키-값 쌍을 저장소에 저장하는 `put(key, value)`메서드
2. 인자로 주어진 키에 매달린 값을 꺼내는 `get(key)`메서드

를 구현하면 된다.

## 📌 문제 이해 및 설계 범위
위 책에서는 특정 상황에서의 설계를 반영한다.
- 키-값 쌍의 크기는 10kb이하이다.
- 큰 데이터를 저장할 수 있어야 한다.
- 높은 가용성을 제공해야 한다. 따라서, 시스템은 설사 장애가 있더라도 빨리 응답해야 한다.
- 높은 규모 확장성을 제공해야 한다. 따라서, 트래픽 양에 따라 자동적으로 서버 증설/삭제가 일어나야 한다.
- 데이터 일관성 수준은 조정이 가능해야 한다.
- 응답 지연시간이 짧아 야한다.


## 📌 단일 서버 키-값 저장소
- 한 대의 서버로 저장소를 설계하는 방법은 키-값 쌍 전부를 해시 테이블로 저장하는 방식이 있다.
  - 빠른 속도를 보장하지만, 메모리의 용량 제한으로 모든 데이터를 둘 수 없을 수 있음.
  - 이에 대한 개선책으로 `데이터 압축`, `자주 쓰이는 데이터만 메모리에 두고 나머지는 디스크 저장` 등을 사용할 수 있다.
    - 하지만, 이 역시 개선책이기 때문에 분명 문제가 있다..!
  - 한 대의 서버로 키-값을 저장할 수 없는 상황이 올 것임! => 분산 키-값 저장소 사용!

## 📌 분산 키-값 저장소
- 분산 해시 테이블이라고도 불리며, 키-값 쌍을 여러 서버에 분산시키는 방법이다.
- 분산 시스템 설계 시에 CAP 정리를 알고 있어야 한다.

### 💡 CAP 정리
```
일관성 : Consistency
가용성 : Availability
파티션 감내 : partition tolerance
```
- 데이터 일관성 : 분산 시스템에 접속하는 모든 클라이언트는 어떤 노드에 접속했느냐에 관계 없이 언제나 같은 데이터를 보게 되어야 한다.
- 가용성 : 분산 시스템에 접속하는 클라이언트는 일부 노드에 장애가 발생하더라도 항상 응답을 받을 수 있어야 한다.
- 파티션 감내 : 네트워크에 파티션이 생기더라도 시스템은 계속 동작하여야 한다.
  
**파티션** ❓
- 두 노드 사이에 통신 장애가 발생하였음을 의미한다.
- 즉, 분산 저장소나 분산 시스템에서 데이터를 저장하고 처리하는 개별 서버나 DB간의 통신 장애가 발생했다!

**CAP 정리는 아래와 같이 이들 가운데 어떤 두가지를 충족하려면 나머지 하나는 반드시 희생되어야 한다는 것을 의미한다.**
![image](https://github.com/user-attachments/assets/6b7507c0-3480-48e5-8a9b-a259752c4498)

1. CP 시스템 : 일관성과 파티션 감내를 지원하는 키-값 저장소. 가용성을 희생.
2. AP 시스템 : 가용성과 파티션 감내를 지원하는 키-값 저장소. 데이터 일관성을 희생.
3. CA 시스템 : 일관성과 가용성을 지원하고, 파티션 감내를 지원하지 않는 키값 저장소. 하지만, 분산 시스템에서 네트워크 장애는 피할 수 없는 일로 여겨지므로 실세계에서는 CA 시스템이 존재하지 않음.

<U>분산 시스템에서 데이터는 보통 여러 노드에 복제되어 보관되는데, 세 대의 복제(replica) 노드 n1, n2, n3에 데이터를 복제하여 보관 되는 상황을 생각해보자.</U>

🔎 **이상적 상태**
![image](https://github.com/user-attachments/assets/f7ff0f96-6149-486f-bc6a-1e00ac2d8838)
- 이상적 환경이라면 네트워크가 파티션되는 상황은 일어나지 않을 것이다.
- n1에 기록된 데이터는 자동적으로 n2와 n3에 복제된다.
- 즉, 데이터 일관성과 가용성도 만족한다.

🔎 **실세계의 분산 시스템**
- 실세계에서는 파티션 문제를 피할 수 없다(네트워크 문제는 피할 수 없다고 여겨지므로).
- 따라서, 일관성과 가용성 사이에서 하나를 선택해야 한다.

![image](https://github.com/user-attachments/assets/b4581485-68c6-499a-8534-bacab1335afa)

- 위의 예제 처럼 n3에 장애가 발생했다고 하였을 때
  - 클라이언트가 n1 or n2에 기록한 데이터가 n3에 전달되지 않았고,
  - n3에 기록되었지만 n1 or n2에 전달되지 않은 데이터가 있다면 오래된 사본을 갖고 있을 것이다.
    
    - **일관성 선택 예(CP 시스템)** : 세 서버 사이에 생길 수 있는 데이터 불일치 문제를 피하기 위해 n1과 n2에 쓰기 연산을 중단 시켜야 한다(가용성 위배). 은행권 시스템과 같은 곳에서 사용
    - **가용성 선택 예(AP 시스템)** : 설사 낡은 데이터를 반환할 위험이 있더라도 계속 읽기 연산을 허용해야 한다. 아울러 n1과 n2는 계속 쓰기 연산을 허용할 것이다.
- 따라서, 어떤것을 선택하든 해당 상황에 맞추어 설계를 하는 것이 중요하다!

**이제 실제 키-값 저장소 구현에 사용될 기술들을 알아본다.**
### 💡 데이터 파티션
- 대규모 어플리케이션의 경우 전체 데이터를 한 대 서버에 욱여넣는 것은 불가능함.
- 따라서, 가장 단순한 해결책은 데이터를 작은 파티션들로 분할한 후 여러 대 서버에 저장하는 것이다. 단, 여기서 문제점이 존재하는데
  - <U>데이터를 여러 서버에 고르게 분산할 수 있는가?</U>
  - <U>노드가 추가되거나 삭제될 때 데이터의 이동을 최소화할 수 있는가?</U>
- 이를 안정 해시 기술을 통해 해결할 수 있다. 
    안정해시를 통해 
  - 규모 확장 자동화 : 시스템 부하에 따라 서버가 자동으로 추가되거나 삭제되도록 만들 수 있다.
  - 다양성 : 각 서버의 용량에 맞게 가상 노드를 설정할 수 있다.
    를 이룰 수 있다.

### 💡 데이터 다중화
- 높은 가용성과 안정성을 확보하기 위해서는 데이터를 N개 서버에 비동기적으로 다중화할 필요가 있다(여기서 N은 튜닝 가능한 값)
- N개의 서버를 설정하는 방법은 해시 링위에 배치한 후 해당 지점으로부터 링을 순회하면서 첫 N개의 서버에 데이터 사본을 보관하는 것이다.
![image](https://github.com/user-attachments/assets/b52ee251-2fbb-41bf-aa9f-0405e633b28a)
- 위와 같은 예제에서 key0은 s1, s2, s3에 복제되어 저장된다.
- 단, 가상 노드를 사용하면 N개의 노드가 대응될 실제 물리 서버의 개수가 N보다 작아질 수가 있으므로 물리 서버를 중복해서 선택하지 않도록 해야한다.

### 💡 데이터 일관성
여러 노드에 다중화된 데이터는 적절한 동기화가 필요하다. 즉, 나뉘어져 있는 데이터를 동기화하는 과정이 필요하다는 뜻.
`정족수 합의 프로토콜`을 사용하면 읽기/쓰기 연산 모두에 일관성을 보장할 수 있다.

**정족수** ❓
- 합의체가 의사를 진행하고 의결하는 데 필요한 최소의 구성원의 출석 수
- 즉, 위에서는 N개의 서버가 읽기 또는 쓰기 연산이 성공했다는 응답을 받아야 성공한 것으로 간주된다.

정의 :
1. N = 사본 개수
2. W = 쓰기 연산에 대한 정족수. 쓰기 연산이 성공한 것으로 간주되려면 적어도 R개의 서버로부터 응답을 받아야 한다.
3. R = 읽기 연산에 대한 정족수. 읽기 연산이 성공한 것으로 간주되려면 적어도 R개의 서버로부터 응답을 받아야 한다.

![image](https://github.com/user-attachments/assets/b2eed663-3029-45bd-a23a-c4a4cdbcad78)
위는 N=3인 경우에 대한 예제이다.
- W, R, N의 값을 정하는 것은 응답 지연과 데이터 일관성 사이의 타협점을 찾는 과정이다.
- **W=1인 상황** : 쓰기 연산이 성공했다고 판단하기 위해 중재자는 최소 한대 서버로부터 쓰기 성공 응답을 받아야 한다는 뜻. 즉, s1으로부터 성공 응답을 받았다면 나머지 응답은 기다릴 필요 없음.
- **W=1 or R=1인 상황** : 중재자는 한 대 서버로부터 응답만 받으면 되니 응답속도는 빠를 것이다.
- **W or R이 1보다 클 때** : 시스템이 보여주는 데이터 일관성의 수준은 향상될 테지만, 중재자의 응답속도는 가장 느린 서버로부터 응답을 기다려야 하므로 느려진다.
- **W+R>N인 경우** : 강한 일관성이 보장된다. 일관성을 보증할 최신 데이터를 가진 노드가 최소 하나는 겹칠 것이기 때문이다(하나의 서버는 W도 ok, R도 ok가 되기 때문)
- **R=1 & W=N** : 빠른 읽기 연산에 최적화된 시스템
- **R=N & W=1** : 빠른 쓰기 연산에 최적화된 시스템
-> 즉, 요구되는 일관성 수준에 따라 W, R, N 값을 조정하면 된다.

**🔎 일관성 모델**
- 데이터 일관성의 수준을 결정하는데 따르는 모델 종류.
1. `강한 일관성(strong consistency)` : 모든 읽기 연산은 가장 최근에 갱신된 결과를 반환한다. 즉, 클라이언트는 절대로 낡은 데이터를 보지 못한다.
2. `약한 일관성(weak consistency)` : 읽기 연산은 가장 최근에 갱신된 결과를 반환하지 못할 수 있다.
3. `결과적 일관성(eventual consistency)` : 약한 일관성의 형태로, 갱신 결과가 결국에는 모든 사본에 반영(즉, 동기화)되는 모델이다.

- 강한 일관성을 이루기 위해서는 모든 사본에 현재 쓰기 연산의 결과가 반영될 때까지, 해당 데이터에 대한 읽기/쓰기를 금지하는 것이다(별로임)
- 따라서, 다이나모 or 카산드라 같은 저장소는 `결과적 일관성 모델`을 택하고 있다.

**결과적 일관성 모델을 따를 경우 쓰기 연산이 병렬적으로 발생하면 시스템에 저장된 값의 일관성이 깨어질 수 있는데, 이 문제는 클라이언트가 해결해야 한다.**

**-> 클라이언트 측에서 데이터의 버전 정보를 활용해 일관성이 깨진 데이터를 읽지 않도록 하는 기법을 추가**

**🔎 비 일관성 해소 기법 : 데이터 버저닝**
- 데이터를 다중화하면 가용성은 높아지지만 사본 간 일관성이 깨질 가능성이 커진다.
- 문제 상황 : 
    ![image](https://github.com/user-attachments/assets/42646bae-c412-4998-88b5-484e1a67f8ec)
    - 위와 같이 n1과 n2에 데이터가 다중화 되어있다고 할 때, 각각의 서버로 부터 다른 수정 요청이 들어오면

    ![image](https://github.com/user-attachments/assets/e637bf82-1515-4ba1-8b86-688cb4927f4b)
    - s1과 s2는 서로 충돌 되는 두 값을 갖게 되었다. 이를 각각 버전 v1, v2라고 할 수 있다.
    - 변경이 이뤄진 후에 원래 값은 무시할 수 있지만 각 서버별 v1과 v2 사이의 충돌은 해결하기 어려워 보인다.
    -> 이를 해결하기 위해 `버저닝(versioning)`과 `벡터 시계(vector clock)`가 만들어졌다.

**버저닝 ❓**
  - 데이터를 변경할 때마다 해당 데이터의 새로운 버전을 만드는 것을 의미. 각 버전의 데이터는 변경 불가능
  - 위 예제에서는 이미 가장 최근에 변경할 데이터를 버저닝 했다고 가정.

**벡터 시계 ❓**
- [서버, 버전]의 순서쌍을 데이터에 매단 것이다. 어떤 버전이 선행 버전인지, 후행 버전인지, 다른 버전과 충돌이 있는지 판별하는데 사용된다.
- 벡터 시계는 `D([S1, v1], [S2, v2], ...)`으로 사용된다. 여기서 D는 데이터, vi는 버전 카운터, Si는 서버 번호이다.
- 만일 데이터 D를 서버 Si에 기록하면, 시스템은 아래 작업중 하나를 수행해야 하는데
    1. [Si, vi]가 있으면 vi를 증가시킨다.
    2. 그렇지 않으면 새 항목 [Si, 1]를 만든다.
- 실제 수행 과정을 보면
    ![image](https://github.com/user-attachments/assets/70502104-caa2-41d0-bffa-f2846d103bf5)
    - 위와 같이 서버 y와 z에서 각각 연산을 처리하고 충돌이 해소된 데이터를 `[Sx, 3]`으로 저장함을 통해 충돌이 일어났음을 감지할 수 있다.
- 벡터 시계를 이용하면 어떤 버전 X가 버전 Y의 이전 버전인지 쉽게 판단할 수 있다.
- D([s0, 1], [s1, 1])은 D([s0, 1], [s1, 2])의 이전 버전이다. 따라서 두 데이터 사이에 충돌이 없다.
  - 즉, 버전 x와 y 사이에 충돌이 있는지 보려면 서버 구성요소보다 작은 값을 가지고 있는지 보면 된다.

🧨 **벡터 시계의 문제점**
1. 충돌 감지 및 해소 로직이 클라이언트에 들어가야 하므로, 클라이언트 구현이 복잡해진다.
2. [서버: 버전]의 순서쌍 개수가 굉장히 빨리 늘어나므로 길이에 임계치를 설정하고, 이 이상으로 길이가 길어지면 벡터 시계에서 제거해야 하는 소요가 필요함. 단, 이렇게 하면 버전 간 선후 관계가 정확하게 결정될 수 없기 때문에 충돌 해소 과정의 효율성이 낮아짐.
-> 단, 대부분의 기업에서 벡터 시계는 적용해도 괜찮은 솔루션이다.

### 💡 장애처리
- 대다수 대규모 시스템에서는 장애는 흔히 발생하는 사건이므로 장애 감지 기법과 장애 해소 전략들에 대해 알아야 한다.

🔎 **장애 감지**
- 분산 시스템에서 한대 서버가 특정 서버의 장애를 보고할 경우에 바로 장애처리를 하지 않는다. 보통 두 대 이상의 서버가 똑같이 장애를 보고해야 실제로 장애가 발생했다고 간주한다.
![image](https://github.com/user-attachments/assets/b90ae0b9-510c-4231-a790-7c25b3ea914b)
- 아래와 같이 모든 노드사이에 멀티캐스팅 채널을 구축하면 좋겠지만, 서버가 많을 때 비효율적이다.
- 따라서, `가십 프로토콜`같은 분산형 장애 감지 솔루션을 사용하는 것이 좋다.

**가십 프로토콜** ❓
- 각 노드는 맴버십 목록을 유지한다. 맴버십 목록은 각 맴버 ID와 박동 카운터 쌍의 목록이다.
- 각 노드는 주기적으로 자신의 박동 카운터를 증가시킴.
- 각 노드는 무작위로 선정된 노드들에게 주기적으로 자기 박동 카운터 목록을 보냄(목록임!!!)
- 박동 카운터 목록을 받은 노드는 맴버십 목록을 최신 값으로 갱신한다(더 큰값)
- 어떤 멤버의 박동 카운터 값이 지정된 시간동안 갱신되지 않으면 해당 멤버는 장애상태로 간주.
  
![image](https://github.com/user-attachments/assets/f3b3b500-ba60-4ccc-925d-76f24d32b9a0)
**즉, 각 서버가 다른 서버로 자신의 멤버십 목록을 랜덤으로 보낼텐데 이때 오랫동안 변하지 않은 상태의 값을 offline(장애)이라 간주.**

**🧨 일시적 장애 처리**
- 가십 프로토콜로 장애를 감지한 시스템은 필요한 조치를 취해야한다.
- 엄격한 정족수 접근법을 쓴다면 읽기와 쓰기 연산을 금지해야 할 것이다.
- 느슨한 정족수 접근법을 쓴다면 가용성을 높일 수 있는데, 쓰기 연산을 할 W개의 건강한 서버와 읽기 연산을 할 R개의 건강한 서버를 해시 링에서 고른다.
  - 네트워크나 서버 문제로 장애 상태인 서버로 가는 요청은 다른 서버가 잠시 맡아 처리한다.(임시 위탁)
  ![image](https://github.com/user-attachments/assets/4cbd91a5-2e88-4537-9605-ac3b7a8269d0)
  - 이후, s2가 복귀되면 s3가 갱신된 데이터를 s2로 인계할 것이다.

**🧨 영구 장애 처리**
- 반-엔트로피 프로토콜을 구현하여 사본들을 동기화한다.
- 반 엔트로피 프로토콜은 사본들을 비교하여 최신 버전으로 갱신하는 과정을 포함한다.
- 사본 간의 일관성이 망가진 상태를 탐지하고 전송 데이터의 양을 줄이기 위해 머클트리를 사용한다.
    **🔎 머클 트리**
    - 각 노드에 그 자식 노드들에 보관된 값의 해시 또는 자식 노드들의 레이블로부터 게산된 해시 값을 레이블로 붙여두는 트리이다.
    아래는 예제이다.
    1. 키 공간을 버킷으로 나눈다.
    ![image](https://github.com/user-attachments/assets/ae00e01a-fcaa-4873-86d7-55a45b2f1958)

    2. 버킷에 포함된 각각의 키에 균등 분포 해시 함수를 적용하여 해시값을 계산한다.
    ![image](https://github.com/user-attachments/assets/ac84775b-7a86-4476-9db9-428808d289db)

    3. 버킷 별로 해시값을 계산한 후, 해당 해시 값을 레이블로 갖는 노드를 만든다.
    ![image](https://github.com/user-attachments/assets/a1184437-0e7b-4018-a69f-cd3c3529bda6)

    4. 자식 노드의 레이블로부터 새로운 해시값을 계산하여, 이진 트리를 상향식으로 구성해간다.
    ![image](https://github.com/user-attachments/assets/5f230fbc-3881-4e32-99bd-2a5e0161ee2f)

    - 이를 사용하여 동기화 해야하는 데이터 양이 실제로 존재하는 차이의 크기에 비례한다.

## 시스템 아키텍처 다이어그램
![image](https://github.com/user-attachments/assets/99865156-3b19-4c32-ba3e-368cf017a84a)
- 클라이언트는 키-값 저장소가 제공하는 get, put와 통신한다.
- 중재자는 클라이언트에게 키-값 저장소에 대한 프록시 역할을 하는 노드이다.
- 노드는 안정 해시의 해시 링위에 분포한다.
- 노드를 자동으로 추가 또는 삭제할 수 있도록, 시스템은 완전히 분산한다.
- 데이터는 여러 노드에 다중화한다.
- 모든 노드가 같은 책임을 지므로, SPOF 는 존재하지 않는다.

![image](https://github.com/user-attachments/assets/8eca8a1a-d678-4753-81a5-d1fea843569d)
-> 완전히 분산된 노드를 채택하였으므로 아래 제시된 기능을 전부 지원해야함!

## 쓰기 요청이 특정 노드에 전달되었을 때 
![image](https://github.com/user-attachments/assets/ebcfdbb5-ebf5-474d-8d2a-45c8fb88b9dc)
1. 쓰기 요청이 커밋 로그 파일에 기록된다.
2. 데이터가 메모리 캐시에 기록된다.
3. 메모리 캐시가 가득차거나 임계치에 도달하면 디스크에 있는 SSTable에 기록된다. <키, 값>의 순서쌍을 정렬된 리스트로 관리하는 테이블이다.

## 읽기 요청이 특정 노드에 전달되었을 때
![image](https://github.com/user-attachments/assets/2c5b431d-40ab-45bd-9760-8b571ad39362)
1. 데이터가 메모리에 없을 경우 블룸 필터로 검사한다.
2. 블룸 필터로 어떤 SStable에 키가 보관되어있는지 알아낸다.
3. SSTable에서 데이터를 가져온다.
4. 해당 데이터를 클라이언트에게 반환한다.


# References
사진 참조 : 
https://donghyeon.dev/%EC%9D%B8%ED%94%84%EB%9D%BC/2022/03/26/%ED%82%A4-%EA%B0%92-%EC%A0%80%EC%9E%A5%EC%86%8C-%EC%84%A4%EA%B3%84/#google_vignette
https://jonghoonpark.com/2023/06/01/key-value-store